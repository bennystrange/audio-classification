{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "GCXIyzo-lwAx",
   "metadata": {
    "id": "GCXIyzo-lwAx"
   },
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d0e628",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27627,
     "status": "ok",
     "timestamp": 1745209727682,
     "user": {
      "displayName": "Matthew Nutt",
      "userId": "01284318922524244496"
     },
     "user_tz": 300
    },
    "id": "f2d0e628",
    "outputId": "c0861590-4805-4e22-c620-f0695e8e555c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.fft import fft, fftfreq\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# in theory, this should automagically detect whether or not you're in colab and set the directory accordingly\n",
    "# if in colab, put link to shared 378final folder in My Drive\n",
    "# if local, make a dataset folder containing the .wav folders in the same directory as the .py file\n",
    "# we won't track dataset folder on Git because it's 2GB Lol.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    dataset_directory = \"/content/drive/My Drive/378final/dataset\"\n",
    "except:\n",
    "    dataset_directory = \"./dataset\"\n",
    "\n",
    "# Hardcoded labels\n",
    "labels = [\"Angry\", \"Disgusted\", \"Fearful\", \"Happy\", \"Neutral\", \"Sad\", \"Suprised\"]\n",
    "# Numpy array makes it easier to make a list from a list of indices\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d6f78",
   "metadata": {
    "id": "837d6f78"
   },
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e44596",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 29918,
     "status": "ok",
     "timestamp": 1745209757603,
     "user": {
      "displayName": "Matthew Nutt",
      "userId": "01284318922524244496"
     },
     "user_tz": 300
    },
    "id": "e0e44596",
    "outputId": "f470f7e8-092e-4010-ccb5-d9363a5170de"
   },
   "outputs": [],
   "source": [
    "\"\"\"Data Exploration\"\"\"\n",
    "\n",
    "# we would like to get a sense of the signal's properties so see what are some good features to extract\n",
    "#j = 1\n",
    "#for emotion in labels:\n",
    "emotion = \"Angry\"\n",
    "for j in range(1, 4):\n",
    "    file_path = f\"{dataset_directory}/{emotion}/{emotion}{j}.wav\"\n",
    "    audio, sampling_rate = librosa.load(file_path, sr=16000)  # load full audio signal\n",
    "\n",
    "    fig,axs = plt.subplots(3,2)\n",
    "\n",
    "    # raw audio waveform\n",
    "    axs[0,0].plot([i/sampling_rate for i in range(audio.shape[0])], audio, color = \"green\")\n",
    "    axs[0,0].set_title(\"Waveform\")\n",
    "    axs[0,0].set_xlabel(\"Time (s)\")\n",
    "    axs[0,0].set_ylabel(\"Amplitude\")\n",
    "    axs[0,0].grid()\n",
    "\n",
    "    # fft\n",
    "    N = len(audio)\n",
    "    yf = fft(audio)\n",
    "    xf = fftfreq(N, 1 / sampling_rate)[:N // 2]\n",
    "    axs[0,1].plot(xf, 2.0 / N * np.abs(yf[0:N // 2]), color = \"blue\")\n",
    "    axs[0,1].set_title(\"FFT\")\n",
    "    axs[0,1].set_xlabel(\"Frequency (Hz)\")\n",
    "    axs[0,1].set_ylabel(\"Amplitude\")\n",
    "    axs[0,1].grid()\n",
    "    axs[0,1].set_xscale(\"log\")\n",
    "\n",
    "    # psd\n",
    "    f, Pxx = welch(audio, sampling_rate, nperseg=1024)\n",
    "    axs[1,0].set_title(\"PSD\")\n",
    "    axs[1,0].set_xlabel(\"Frequency (Hz)\")\n",
    "    axs[1,0].set_ylabel(\"Power Spectral Density\")\n",
    "    axs[1,0].grid()\n",
    "    axs[1,0].semilogy(f, Pxx, color = \"orange\")\n",
    "    axs[1,0].set_xscale(\"log\")\n",
    "\n",
    "    # mfcc\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sampling_rate, n_mels=128) #can experiment with n_mels (try 22-128)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mfccs = librosa.feature.mfcc(S=mel_spec_db, sr=sampling_rate, n_mfcc=12) #experiment with n_mfcc?? (8-20)\n",
    "\n",
    "    delta_mfccs  = librosa.feature.delta(mfccs)\n",
    "    delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "\n",
    "    axs[1,1].imshow(mfccs, cmap=\"hot\", interpolation=\"nearest\", origin=\"lower\")\n",
    "    axs[1,1].set_title(\"MFCCs\")\n",
    "    axs[1,1].set_xlabel(\"Time\")\n",
    "    axs[1,1].set_ylabel(\"MFCC Coefficients\")\n",
    "\n",
    "    axs[2,0].imshow(delta_mfccs, cmap=\"hot\", interpolation=\"nearest\", origin=\"lower\")\n",
    "    axs[2,0].set_title(\"Derivative of MFCCs\")\n",
    "    axs[2,0].set_xlabel(\"Time\")\n",
    "    axs[2,0].set_ylabel(\"MFCC Coefficients\")\n",
    "\n",
    "    axs[2,1].imshow(delta2_mfccs, cmap=\"hot\", interpolation=\"nearest\", origin=\"lower\")\n",
    "    axs[2,1].set_title(\"Second Derivative of MFCCs\")\n",
    "    axs[2,1].set_xlabel(\"Time\")\n",
    "    axs[2,1].set_ylabel(\"MFCC Coefficients\")\n",
    "\n",
    "    # show all at once\n",
    "    fig.suptitle(f\"Features of {emotion}{j}.wav\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5639a729",
   "metadata": {
    "id": "5639a729"
   },
   "source": [
    "Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc81121",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745209757606,
     "user": {
      "displayName": "Matthew Nutt",
      "userId": "01284318922524244496"
     },
     "user_tz": 300
    },
    "id": "bbc81121"
   },
   "outputs": [],
   "source": [
    "\"\"\"Feature Extraction (per audio signal)\"\"\"\n",
    "def extract_features(file_path, standard_audio_size=3):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        # Apparently the sampling rate differs between files, so we fix it here\n",
    "        audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "        # Pad/truncate to get standard length (so features are consistent length)\n",
    "        padded_audio = librosa.util.fix_length(audio, size=standard_audio_size*sampling_rate)\n",
    "\n",
    "        # Extract features\n",
    "        mel_spec = librosa.feature.melspectrogram(y=padded_audio, sr=sampling_rate, n_mels=128, hop_length=1024, n_fft=4096)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mfccs = librosa.feature.mfcc(S=mel_spec_db, sr=sampling_rate, n_mfcc=12)\n",
    "        delta_mfccs  = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "\n",
    "        all_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs]).flatten(order='F')\n",
    "        return all_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183888af",
   "metadata": {
    "id": "183888af"
   },
   "source": [
    "Extracting Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b4b443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181451,
     "status": "ok",
     "timestamp": 1745209939052,
     "user": {
      "displayName": "Matthew Nutt",
      "userId": "01284318922524244496"
     },
     "user_tz": 300
    },
    "id": "19b4b443",
    "outputId": "589f999a-05fb-4f82-9c0b-59ffecec06a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Extracting Train Data---\n",
      "Extracting: Angry\n",
      "Extracting: Disgusted\n",
      "Extracting: Fearful\n",
      "Extracting: Happy\n",
      "Extracting: Neutral\n",
      "Extracting: Sad\n",
      "Extracting: Suprised\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extracting Train Data\"\"\"\n",
    "print(\"---Extracting Train Data---\")\n",
    "\n",
    "# Load dataset and extract features\n",
    "training_samples = []\n",
    "training_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"Extracting: {label}\")\n",
    "    for num, file_name in enumerate(os.listdir(f\"{dataset_directory}/{label}\")):\n",
    "        #if (num < 200): # comment this out this to use ALL files\n",
    "            file_path = f\"{dataset_directory}/{label}/{file_name}\"\n",
    "            training_sample = extract_features(file_path)\n",
    "            if training_sample is not None:\n",
    "                training_samples.append(training_sample)\n",
    "                training_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays, encode labels\n",
    "training_samples = np.array(training_samples)\n",
    "training_labels = np.array(training_labels)\n",
    "training_labels_encoded = [labels.tolist().index(label) for label in training_labels]\n",
    "training_labels_encoded = np.array(training_labels_encoded)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "training_samples_scaled = scaler.fit_transform(training_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351a6d0",
   "metadata": {},
   "source": [
    "Extracting Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de65188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Extracting Test Data---\n",
      "25% complete\n",
      "50% complete\n",
      "75% complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extracting Test Data\"\"\"\n",
    "print(\"---Extracting Test Data---\")\n",
    "\n",
    "# for status updates\n",
    "num_test = len(os.listdir(f\"{dataset_directory}/Test\"))\n",
    "checkpoints = []\n",
    "for i in range(1, 5):\n",
    "    checkpoints.append(round(i*num_test/4))\n",
    "\n",
    "test_samples = []\n",
    "test_nums = []\n",
    "for i, file_name in enumerate(os.listdir(f\"{dataset_directory}/Test\")):\n",
    "    for point in checkpoints:\n",
    "        if i == point:\n",
    "            print(f\"{int(25*(checkpoints.index(point)+1))}% complete\")\n",
    "    file_path = f\"{dataset_directory}/Test/{file_name}\"\n",
    "    test_sample = extract_features(file_path)\n",
    "    if test_sample is not None:\n",
    "        test_samples.append(test_sample)\n",
    "        test_nums.append(int(file_name.split('.')[0]))\n",
    "\n",
    "# Scale with same scaler as train data\n",
    "test_samples = np.array(test_samples)\n",
    "test_samples_scaled = scaler.transform(test_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7134dd",
   "metadata": {
    "id": "8d7134dd"
   },
   "source": [
    "Training Model and Predicting (SVM w/ PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87cd08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8578,
     "status": "ok",
     "timestamp": 1745209947626,
     "user": {
      "displayName": "Matthew Nutt",
      "userId": "01284318922524244496"
     },
     "user_tz": 300
    },
    "id": "be87cd08",
    "outputId": "b52dc4e1-6985-4dee-d2fe-501d8680ccc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Validating Model---\n",
      "Split #1\n",
      "Split #2\n",
      "Split #3\n",
      "Split #4\n",
      "Split #5\n",
      "Average Accuracy: 60.43%\n",
      "---Training Final Model---\n",
      "---Making Predictions---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training Model and Predicting\"\"\"\n",
    "print(\"---Validating Model---\")\n",
    "# NOTE: It seems like the TAs wanted to reduce overfitting by using PCA. However, it also seems that, at least for the full dataset,\n",
    "#       leaving higher variance in the data results in better validation scores!\n",
    "#       And after some tests, turns out that not using PCA at all is slower, but nets a 20% accuracy bonus above 99% variance!\n",
    "\n",
    "# SVM Model, preprocessing, and cross-validation initialization\n",
    "svm_clf = SVC(kernel=\"rbf\", C=1, gamma=\"auto\")\n",
    "pca = PCA(n_components=0.99) # preserve x% of variance in data\n",
    "kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for i, (train_index, val_index) in enumerate(kf.split(training_samples_scaled, training_labels_encoded)):\n",
    "    print(f\"Split #{i+1}\")\n",
    "    X_train, X_val = training_samples_scaled[train_index], training_samples_scaled[val_index]\n",
    "    y_train, y_val = training_labels_encoded[train_index], training_labels_encoded[val_index]\n",
    "    #PCA_X_train = pca.fit_transform(X_train)\n",
    "    PCA_X_train = X_train\n",
    "    #PCA_X_val = pca.transform(X_val)\n",
    "    PCA_X_val = X_val\n",
    "    svm_clf.fit(PCA_X_train, y_train)\n",
    "    y_pred = svm_clf.predict(PCA_X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"---Training Final Model---\")\n",
    "# seems like more data requires higher variance\n",
    "pca = PCA(n_components=0.99) # preserve x% of variance in data\n",
    "#PCA_training_samples = pca.fit_transform(training_samples_scaled)\n",
    "PCA_training_samples = training_samples_scaled\n",
    "svm_clf.fit(PCA_training_samples, training_labels_encoded)\n",
    "\n",
    "\n",
    "\n",
    "print(\"---Making Predictions---\")\n",
    "\n",
    "# Transform features using PCA\n",
    "#PCA_test_samples = pca.transform(test_samples_scaled)\n",
    "PCA_test_samples = test_samples_scaled\n",
    "\n",
    "# Predict using the SVM (with decoded predictions)\n",
    "PCA_test_pred = svm_clf.predict(PCA_test_samples)\n",
    "PCA_test_pred_labels = labels[PCA_test_pred]\n",
    "PCA_test_pred_labels_sorted = [test_label for _, test_label in sorted(zip(test_nums, PCA_test_pred_labels))]\n",
    "\n",
    "# Save to a CSV for submission to Kaggle\n",
    "with open(\"svm_PCA.csv\", 'w', newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filename\", \"emotion\"])\n",
    "    for i, emotion in enumerate(PCA_test_pred_labels_sorted):\n",
    "        writer.writerow([f\"{i+1}.wav\", emotion])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3dMu_S-lwBA",
   "metadata": {
    "id": "h3dMu_S-lwBA"
   },
   "source": [
    "Training Model and Predicting (SVM w/ RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80180d63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80180d63",
    "outputId": "d1337d2c-e328-4f54-9503-5db6ec2b7c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Training Model---\n",
      "Fitting estimator with 1692 features.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reducing Overfitting through Recursive Feature Extraction (RFE)\n",
    "\n",
    "RFE is a feature selection method that fits a model with the\n",
    "existing features and removes the weakest feature. This fitting process is repeated until a specified\n",
    "number of features remains. In this case, the model is initially fit with all the combined features. RFE ranks the features by their importance to the predictive accuracy. The least important\n",
    "feature is removed recursively. The model is then refitted with the new, reduced set of features. This\n",
    "process is iterated until the specified number of features is reached.\n",
    "\"\"\"\n",
    "VALIDATION = True\n",
    "\n",
    "if VALIDATION:\n",
    "    print(\"---Validating Model---\")\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_samples_scaled, training_labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Determine if you should use the dual formulation\n",
    "    use_dual = X_train.shape[0] > X_train.shape[1]  # True if more samples than features\n",
    "\n",
    "    # Initialize the LinearSVC model for RFE\n",
    "    linear_svc = LinearSVC(dual=use_dual, max_iter=20000, tol=1e-4)\n",
    "\n",
    "    # Feature selection with RFE using LinearSVC\n",
    "    rfe = RFE(estimator=linear_svc, n_features_to_select=20, step=250, verbose=3)\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Transform features using RFE\n",
    "    RFE_X_train = rfe.transform(X_train)\n",
    "    RFE_X_val = rfe.transform(X_val)\n",
    "\n",
    "    # Train the final model using the RBF kernel with selected features\n",
    "    model = SVC(kernel=\"rbf\", C=1, decision_function_shape=\"ovo\", gamma=\"scale\")\n",
    "    model.fit(RFE_X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set with the reduced feature set\n",
    "    y_pred = model.predict(RFE_X_val)\n",
    "    validation_accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy}\")\n",
    "else:\n",
    "    print(\"---Training Final Model---\")\n",
    "\n",
    "    # Determine if you should use the dual formulation\n",
    "    use_dual = training_samples_scaled.shape[0] > training_samples_scaled.shape[1]  # True if more samples than features\n",
    "\n",
    "    # Initialize the LinearSVC model for RFE\n",
    "    linear_svc = LinearSVC(dual=use_dual, max_iter=20000, tol=1e-4)\n",
    "\n",
    "    # Feature selection with RFE using LinearSVC\n",
    "    rfe = RFE(estimator=linear_svc, n_features_to_select=20, step=250, verbose=3)\n",
    "    rfe.fit(training_samples_scaled, training_labels_encoded)\n",
    "\n",
    "    # Transform features using RFE\n",
    "    RFE_training_samples_scaled = rfe.transform(training_samples_scaled)\n",
    "\n",
    "    # Train the final model using the RBF kernel with selected features\n",
    "    model = SVC(kernel=\"rbf\", C=1, decision_function_shape=\"ovo\", gamma=\"scale\")\n",
    "    model.fit(RFE_training_samples_scaled, training_labels_encoded)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"---Making Predictions---\")\n",
    "\n",
    "    # Transform features using RFE\n",
    "    RFE_test_samples = rfe.transform(test_samples_scaled)\n",
    "\n",
    "    # Predict using the SVM (with decoded predictions)\n",
    "    RFE_test_pred = model.predict(RFE_test_samples)\n",
    "    RFE_test_pred_labels = labels[RFE_test_pred]\n",
    "    RFE_test_pred_labels_sorted = [test_label for _, test_label in sorted(zip(test_nums, RFE_test_pred_labels))]\n",
    "\n",
    "    # Save to a CSV for submission to Kaggle\n",
    "    with open(\"svm_RFE.csv\", 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"emotion\"])\n",
    "        for i, emotion in enumerate(RFE_test_pred_labels_sorted):\n",
    "            writer.writerow([f\"{i+1}.wav\", emotion])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Note on Overfitting:\n",
    "Note how we tried to mitigate the overfitting problem explicitly at the end through RFE but also implicitly at several prior points in the data science pipeline. We used PCA to capture 95% variance and thus eliminate highly specfic dimensions in feature space that could potentially cause the model to overfit. We also used K-fold cross validation instead of a simple train test split in order to not make training dependent on just one random split of the available data.\n",
    "\n",
    "Other Ways to Address Overfitting:\n",
    "* Splitting train validation ratio: The dataset this year is pretty large. The typical train validation split is 80% and 20%. You can always experiment with different ratios to find better performance. However, increasing or decreasing one side too much can lead to overfitting too much to the validation data or underfitting to the validation data.\n",
    "* Grid search vs random search:\n",
    "  * Both are ways to tune hyperparameters of your model for better performance. However, both have benefits and downsides.\n",
    "  * Grid search tries all combinations of hyperparameters in the range of values that you provide. Random search randomly samples hyperparameters in the range of values that you provide.\n",
    "  * General fast rule of thumb: Use grid search when there are less hyperparameters that also have more correlation with each other. Use random search when there are too many parameters without correlation with each other to see better trends first.\n",
    "* Early stopping: Use a plot to monitor the validation set performance. When the validation stops improving is when you stop training the model early before it reaches the end of the whole training iterations specified.\n",
    "* Regularization: Introduce a penalty term in the loss function that prevents over-emphasis and weights on a specific parameter. Different regularization methods were covered in class: L1 (Lasso) and L2 (Ridge).\n",
    "* Data augmentation: Create more data that is slightly changed from the given train set if you feel you want more training data.\n",
    "* Too many features: feature selection and dimensionality reduction like PCA.\n",
    "\n",
    "Other general tips:\n",
    "*   Consider using raw audio signals vs extracted features of audio signals vs images to train your models.\n",
    "*   Consider the implications about your data based on the test accuracies of models. For example, if your SVM has a poor accuracy (despite correct implementation), then that most likely implies that your data is probably ont linearly separable. In that case try using kernels or even better for your second model - neural nets!\n",
    "* CNNs (very common for such tasks, although not SOTA) can be used on 1D (WaveNet), 2D (ResNet), or 3D data (many times people assume it's only for 2D images).\n",
    "* Transformers are SOTA but take very long to train, especially with Colab's free GPU, so keep that in mind.\n",
    "* Loading Labels (IMPORTANT): The data for this year has each emotion and their files in a separate folder each without a created train.csv to download. So, please create a train.csv with one column being the filename and second column being the corresponding emotion based on which folder the file is in.\n",
    "  * NOTE: the emotion \"Surprised\" is spelled as \"Suprised\" so use \"Suprised\" across everything\n",
    "* Submission file format: Please be aware of the format and follow the format given on Kaggle of two columns with the same column names and format with the emotions having the first letter capitalized.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
